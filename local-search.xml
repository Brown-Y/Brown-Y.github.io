<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Linear Regression</title>
    <link href="/2025/04/14/testfile/"/>
    <url>/2025/04/14/testfile/</url>
    
    <content type="html"><![CDATA[<h1 id="线性回归">线性回归</h1><p>回归：为自变量与因变量之间进行建模</p><p>机器学习中大多数任务与预测有关，预测问题可以分为：<strong>分类</strong>，<strong>回归</strong>两大类</p><h2 id="基本元素">基本元素：</h2><p>训练数据集/训练集</p><p>每行数据、数据点、样本</p><p>标签、目标：试图预测的对象</p><p>特征、协变量：预测所依据的自变量</p><p>样本数：<span class="math inline"><em>n</em></span></p><h2 id="线性模型">线性模型</h2><p><spanclass="math display"><strong>ŷ</strong> = <strong>X</strong><strong>w</strong> + <em>b</em></span></p><p>在这个求和过程中将使用广播机制，<spanclass="math inline"><strong>w</strong></span>表示权重矩阵</p><h2 id="损失函数">损失函数</h2><p><em>损失函数</em>（lossfunction）能够量化目标的<em>实际</em>值与<em>预测</em>值之间的差距。回归问题中最常用的损失函数是平方误差函数。当样本𝑖的预测值为𝑦^(𝑖)，其相应的真实标签为𝑦(𝑖)时，平方误差可以定义为以下公式： <span class="math display">$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} -y^{(i)}\right)^2.$$</span> 常数1/2将在后续方便进行求导计算。</p><p><img src="/img/testfile1/fit-linreg.png" /></p><p>为了度量模型在整个数据集上的质量，我们需计算在训练集𝑛个样本上的损失均值（也等价于求和）<span class="math display">$$L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b)=\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top\mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$</span> 将损失关于𝑤的导数设为0，得到解析解： <spanclass="math display"><strong>w</strong><sup>*</sup> = (<strong>X</strong><sup>⊤</sup><strong>X</strong>)<sup>−1</sup><strong>X</strong><sup>⊤</sup><strong>y</strong>.</span>像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。</p><h2 id="随机梯度下降">随机梯度下降</h2><p>在我们无法得到解析解的情况下，我们仍然可以使用<strong>梯度下降</strong>的方法有效地训练模型。</p><p>梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度）。但实际中的执行可能会<strong>非常慢</strong>：因为在每一次更新参数之前，我们必须遍历整个数据集。</p><p>因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做<strong>小批量随机梯度下降</strong>,其步骤如下：</p><ol type="1"><li>随机抽样一个小批量<span class="math inline">ℬ</span></li><li>计算小批量的平均损失关于模型参数的导数（梯度）</li><li>将梯度乘以一个预先确定的正数𝜂，并从当前参数的值中减掉</li></ol><p><span class="math display">$$(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|}\sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)}l^{(i)}(\mathbf{w},b).$$</span></p><p>·|𝐵|表示每个小批量中的样本数（批量大小）</p><p>·<spanclass="math inline"><em>η</em></span>表示<strong>学习率</strong></p><p>批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。这些可以调整但不在训练过程中更新的参数称为<strong>超参数</strong>。</p><p><em>调参</em>（hyperparameter tuning）是选择超参数的过程。</p><p>超参数通常是我们根据训练迭代结果来调整的，而训练迭代结果是在独立的<em>验证数据集</em>（validationdataset）上评估得到的。</p><p>事实上，难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失，这一挑战被称为<em>泛化</em>（generalization）。</p><h2 id="线性回归模型的实现">线性回归模型的实现</h2><p>在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。为了实现这一点，需要我们对计算进行<strong>矢量化</strong>，从而利用线性代数库，而不是在Python中编写开销高昂的for循环。</p><h2 id="从线性回归到深度网络">从线性回归到深度网络</h2><p>线性回归是一个单层神经网络。</p><p><img src="/img/testfile1/singleneuron.png" /></p><p>该图只显示连接模式，即只显示每个输入如何连接到输出，隐去了权重和偏置的值。</p><p>对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连，我们将这种变换称为<strong>全连接层</strong>（fully-connectedlayer）或称为<strong>稠密层</strong>（dense layer）。</p>]]></content>
    
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>note</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
